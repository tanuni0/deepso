{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_function import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "class PSOGNN(nn.Module):\n",
    "    def __init__(self, node_input_dim, hidden_dim=32):\n",
    "        super(PSOGNN, self).__init__()\n",
    "        self.node_input_dim = node_input_dim\n",
    "        self.conv1 = gnn.GCNConv(node_input_dim, hidden_dim)\n",
    "        self.conv2 = gnn.GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, 3)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        B, n, d = x.size()\n",
    "        x = x.view(B * n, d)\n",
    "\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            x = F.relu(self.conv1(x, edge_index))\n",
    "            x = F.relu(self.conv2(x, edge_index))\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        x = x.view(B, n, 3)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSO():\n",
    "    def __init__(self, X, W, C1, C2, function, lower_bound, upper_bound, device='cuda', patience=5):\n",
    "        self.device = device  \n",
    "        self.X = X.to(device)\n",
    "        self.W = W.to(device)\n",
    "        self.C1 = C1.to(device)\n",
    "        self.C2 = C2.to(device)\n",
    "        self.func = function\n",
    "        self.lower_bound = torch.as_tensor(lower_bound, device=device).float()\n",
    "        self.upper_bound = torch.as_tensor(upper_bound, device=device).float()\n",
    "        self.num_particle, self.dim = X.shape\n",
    "        self.V = torch.zeros((self.num_particle, self.dim), device=device)\n",
    "        self.P = X.clone().to(device)\n",
    "        self.P_best = torch.full((self.num_particle, 1), float('inf'), device=device)\n",
    "        self.G = None\n",
    "        self.global_best = torch.tensor(float('inf'), device=device)\n",
    "        self.global_best_history = []\n",
    "        self.patience = patience\n",
    "\n",
    "    def initial_global_best(self):\n",
    "        fitnesses = self.func(self.X)\n",
    "        min_fitness, min_id = torch.min(fitnesses, dim=0)\n",
    "        self.global_best = min_fitness\n",
    "        self.G = self.X[min_id].clone().to(self.device)\n",
    "        return self.G\n",
    "\n",
    "    def update_position(self):\n",
    "        if self.G is None:  \n",
    "            self.G = self.initial_global_best()\n",
    "\n",
    "        random_tensor_1 = torch.rand((self.num_particle, 1), device=self.device)\n",
    "        random_tensor_2 = torch.rand((self.num_particle, 1), device=self.device)\n",
    "\n",
    "        new_velocity = (\n",
    "            self.W.reshape(-1, 1) * self.V +\n",
    "            random_tensor_1 * self.C1.reshape(-1, 1) * (self.P - self.X) +\n",
    "            random_tensor_2 * self.C2.reshape(-1, 1) * (self.G - self.X)\n",
    "        )\n",
    "\n",
    "        new_position = self.X + new_velocity\n",
    "        new_position = torch.clamp(new_position, self.lower_bound, self.upper_bound)\n",
    "        self.V = new_velocity.clone().detach()\n",
    "        self.X = new_position.clone().detach()\n",
    "        return new_position\n",
    "\n",
    "    def update_fitness(self, new_position):\n",
    "        fitnesses = self.func(new_position).reshape(-1, 1)\n",
    "        fitnesses_no_grad = fitnesses.detach()\n",
    "\n",
    "        improve = fitnesses_no_grad < self.P_best\n",
    "        self.P_best = torch.where(improve, fitnesses_no_grad, self.P_best)\n",
    "        self.P = torch.where(improve, new_position, self.P)\n",
    "        \n",
    "        min_fitness, min_id = torch.min(fitnesses_no_grad, dim=0)\n",
    "        \n",
    "        if self.global_best > min_fitness:\n",
    "            self.global_best = min_fitness\n",
    "            self.G = new_position[min_id].clone().to(self.device)\n",
    "        \n",
    "        self.global_best_history.append(self.global_best.item())\n",
    "        return torch.mean(fitnesses)\n",
    "\n",
    "    def run_step(self):\n",
    "        if self.G is None:\n",
    "            self.G = self.initial_global_best()\n",
    "        \n",
    "        new_position = self.update_position()\n",
    "        mean_fitness = self.update_fitness(new_position)\n",
    "        print(new_position)\n",
    "        print(mean_fitness)\n",
    "        \n",
    "        return new_position, mean_fitness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 1, 'func_type': 'ackley', 'params': [21.240033376567105, 0.20560771459870464, 7.294585386829872]}\n"
     ]
    }
   ],
   "source": [
    "splited_path = r'A:\\Code\\deepso\\splited_data_no.pkl'\n",
    "with open(splited_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "train_set = dataset['train']\n",
    "test_set = dataset['test']\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(dataset, batch_size):\n",
    "    print(\"Dataset type:\", type(dataset))\n",
    "    if not isinstance(dataset, list):\n",
    "        dataset = list(dataset)\n",
    "    \n",
    "    random.shuffle(dataset)\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i + batch_size]\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_particle = 10\n",
    "padding_dim = 100\n",
    "lower_bound = -50\n",
    "upper_bound = 50\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def initial_padding_batch(batch):\n",
    "    x_list = []\n",
    "    for func in batch:\n",
    "        dim = func['dim']\n",
    "        x = lower_bound + (upper_bound - lower_bound) * torch.rand((num_particle, dim), device=device)\n",
    "        if dim < padding_dim:\n",
    "            x_padding = F.pad(x, (0, padding_dim - dim))\n",
    "            x_list.append(x_padding)\n",
    "    return x_list\n",
    "\n",
    "def padding_batch(x_list, batch):\n",
    "    x_list_update = []\n",
    "    for x_ori, func in zip(x_list, batch):\n",
    "        dim = func['dim']\n",
    "        if dim < padding_dim:\n",
    "            x_padding = F.pad(x_ori, (0, padding_dim - dim))\n",
    "            x_list_update.append(x_padding)\n",
    "    return x_list_update\n",
    "\n",
    "def unpadding(x_list, batch):\n",
    "    x_ori = []\n",
    "    for x_padded, func in zip(x_list, batch):\n",
    "        dim = func['dim']\n",
    "        x_original = x_padded[:, :dim]\n",
    "        x_ori.append(x_original)\n",
    "    return x_ori\n",
    "\n",
    "def train(dataset, max_step, num_epochs, batch_size, hidden_dim, num_particle, padding_dim, device='cuda'):\n",
    "    epoch_losses = []\n",
    "    model = PSOGNN(node_input_dim=padding_dim, hidden_dim=hidden_dim).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        function_loader = create_batches(dataset, batch_size)\n",
    "\n",
    "        for batch in function_loader:\n",
    "            print(f\"Batch {batch_count+1}\")\n",
    "            \n",
    "            for step in range(max_step):\n",
    "                if step == 0:\n",
    "                    x_list = initial_padding_batch(batch)\n",
    "                else:\n",
    "                    x_list = padding_batch(x_list, batch)\n",
    "                \n",
    "                x_padding_all = torch.stack(x_list, dim=0).to(device)\n",
    "                x_ori = unpadding(x_list, batch)\n",
    "\n",
    "                edge_index = torch.combinations(torch.arange(num_particle, device=device), r=2).t().contiguous().to(device)\n",
    "                batch_indices = torch.arange(batch_count, device=device).repeat_interleave(num_particle)\n",
    "                \n",
    "                output = model(x_padding_all, edge_index, batch_indices)\n",
    "                \n",
    "                output = output\n",
    "                \n",
    "                W_all = output[:, :, 0]\n",
    "                C1_all = output[:, :, 1]\n",
    "                C2_all = output[:, :, 2]\n",
    "\n",
    "                for index, func in enumerate(batch):\n",
    "                    dim = func['dim']\n",
    "                    func_type = func['func_type']\n",
    "                    params = func['params']\n",
    "                    \n",
    "                    def function(x):\n",
    "                        function_instance = Function.get_function(func_type, x, params)\n",
    "                        return function_instance.evaluate_function()\n",
    "                    \n",
    "                    W = W_all[index]\n",
    "                    C1 = C1_all[index]\n",
    "                    C2 = C2_all[index]\n",
    "                    X = x_ori[index].to(device)\n",
    "\n",
    "                    pso = PSO(X, W, C1, C2, function, lower_bound, upper_bound, device=device, patience=5)\n",
    "                    new_position, mean_fitness = pso.run_step()\n",
    "                    \n",
    "                    x_list[index] = new_position.detach().to(device)\n",
    "\n",
    "                    loss = mean_fitness\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_loss = total_loss + loss.item()\n",
    "\n",
    "\n",
    "            print(f\"Batch {batch_count+1} Loss: {total_loss / max_step}\")\n",
    "            batch_count += 1\n",
    "\n",
    "        epoch_loss = total_loss / len(function_loader)\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch+1} Time: {time.time() - epoch_start_time:.2f} seconds\")\n",
    "\n",
    "    print(f\"Total Training Time: {time.time() - total_start_time:.2f} seconds\")\n",
    "    print(\"Training completed.\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', linestyle='-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Dataset type: <class 'list'>\n",
      "Batch 1\n",
      "tensor([[-2.5789e+01, -2.5441e+01, -2.5964e+01, -3.0763e+01,  1.4347e+01,\n",
      "         -3.4648e+01,  1.7637e+01,  3.2972e+01,  8.1781e-01, -1.8739e+01,\n",
      "         -2.7062e+01,  1.9260e+01, -1.9750e+01, -1.9799e+01,  1.6194e+01,\n",
      "          3.7205e+01, -4.8106e+00, -2.5469e+01,  4.3534e+01,  4.1399e+01,\n",
      "          1.6522e+01,  9.3137e+00,  3.7459e+01, -2.1194e+01,  2.0400e+01,\n",
      "          1.4151e+00, -2.8346e+01, -4.0732e+01, -2.8191e+01,  7.8747e+00,\n",
      "         -1.9207e+00,  3.1453e+01,  2.5606e+01,  3.0020e+01, -2.9246e+01,\n",
      "          3.3737e+01,  1.9263e+01, -2.0139e+01,  2.5457e+01,  3.9150e+01,\n",
      "         -1.7901e+01,  4.4571e+01, -3.7281e+01, -2.5012e+01, -9.2577e+00,\n",
      "          1.4868e+01,  3.3173e+01, -9.0441e+00,  2.1142e+01, -3.2469e+01,\n",
      "         -2.4889e+01,  2.9560e+01,  2.0515e+01,  3.3779e+01,  3.1846e+01,\n",
      "          2.5419e+01, -2.8508e+01, -2.3326e+01,  2.8751e+01,  3.6764e+00,\n",
      "         -1.9809e+01,  3.2678e+01, -2.6581e+01,  7.8518e+00, -2.7530e+01,\n",
      "         -1.6281e+01,  2.7920e+01,  3.2723e+01, -2.2770e+01, -1.1371e+01,\n",
      "          1.2088e+01, -1.2076e+01, -2.6574e+00,  3.8195e+00,  3.3552e+00,\n",
      "         -2.9892e+01,  2.3518e+01,  3.3260e+01, -1.5731e+01, -1.2047e+01,\n",
      "          1.2080e+01],\n",
      "        [ 4.7319e+00, -3.9179e+01,  1.3467e+01, -1.7628e+01, -3.2521e+01,\n",
      "         -4.4493e+00, -3.1411e+00,  8.6210e+00,  3.1484e+01, -1.9435e+01,\n",
      "          4.7083e+00, -3.7547e+01, -9.7154e-01, -1.4586e+01,  1.3520e+01,\n",
      "          1.1271e+01, -1.6181e+01,  3.6522e+01,  2.4249e+01,  2.7399e+01,\n",
      "         -1.3959e+01, -1.9311e+00,  4.0361e+01, -2.1971e+01,  2.3145e+01,\n",
      "         -1.9949e+01,  1.9098e+00, -1.2117e+01,  1.5205e+01, -1.6694e+01,\n",
      "         -1.8957e+01, -2.6602e+01,  9.4761e-01, -4.1574e+00, -6.0175e+00,\n",
      "         -3.6069e+00,  9.4299e+00,  2.5316e-02, -6.5757e+00,  7.0754e+00,\n",
      "         -2.3659e+01,  2.5314e+01, -2.1393e+01,  6.2093e+00,  9.2639e+00,\n",
      "         -1.2162e+01,  2.7018e+00,  2.2132e+01,  1.7254e+01, -3.1819e+01,\n",
      "          7.6643e+00, -3.4128e+01,  2.2109e+01, -4.8201e+00,  1.2272e+01,\n",
      "          1.5311e+01, -6.5551e+00, -2.6980e+01,  1.0106e+01, -9.5246e+00,\n",
      "          7.2067e+00,  1.9440e+01,  3.7648e-01,  2.1861e+01,  4.0915e+01,\n",
      "          5.7424e-01,  9.8288e+00, -2.7338e+01,  2.8644e+01, -2.1133e+00,\n",
      "         -1.7701e+01,  8.1236e+00, -4.2022e+00, -2.5134e+00, -2.3768e+01,\n",
      "         -2.6833e+01,  1.5023e+01, -1.6022e+01,  3.2576e+01,  1.7353e+01,\n",
      "         -2.0626e+01],\n",
      "        [ 1.4826e+01, -2.7763e+01,  2.3472e+01,  3.1190e+01,  1.7780e+01,\n",
      "          3.3800e+01,  1.1039e+01,  2.6056e+01,  1.8158e+01, -4.0672e+01,\n",
      "         -3.9013e+01, -4.6666e+01, -2.6351e-01, -1.8893e+00,  1.6488e+01,\n",
      "         -2.4367e+01, -1.5832e+01,  3.0219e+01,  3.5446e+01,  3.6160e+01,\n",
      "         -4.3968e+00, -2.6828e+01, -1.9673e+01, -1.1561e+01,  2.8542e+01,\n",
      "         -2.1546e-01,  1.4918e+01, -2.6344e+01,  4.6070e+01,  3.7887e+01,\n",
      "         -4.1375e+01, -2.2832e+01,  4.1579e+01, -1.3126e+01, -2.6247e+01,\n",
      "         -2.4150e+01, -6.1577e+00, -1.7427e+01,  2.9933e+01,  6.2536e+00,\n",
      "          3.5639e+01,  9.9524e+00,  3.5386e+01, -1.8500e+01, -3.8507e+01,\n",
      "         -2.3604e+01,  1.8636e+01, -1.0146e+01, -5.1525e+00, -3.3530e+01,\n",
      "         -2.1217e+01, -4.7207e+01, -6.6635e+00, -2.3113e+00,  3.1214e+01,\n",
      "         -3.6221e+01, -1.2247e+01, -3.6892e+01,  2.3382e+00, -1.9990e+01,\n",
      "         -7.2432e+00,  1.5329e+01, -2.5774e+01, -1.5459e+01,  1.3246e+01,\n",
      "         -1.6287e+01, -4.5589e+00,  2.8210e+01, -2.1254e+01,  1.8356e+01,\n",
      "         -2.2160e+01, -1.6935e+01,  2.2928e+01, -3.4166e+01, -1.2951e+01,\n",
      "         -1.7565e+01,  3.1611e+01,  2.4301e+01, -1.4421e+01,  1.0230e+01,\n",
      "          3.0617e+01],\n",
      "        [-3.1061e+01, -4.3943e+01,  2.6364e+01,  4.0686e+00, -5.9561e+00,\n",
      "          1.5544e+01, -6.0937e+00,  2.9933e+01,  1.8439e+00,  9.3415e+00,\n",
      "         -3.3748e+00, -2.2979e+01,  1.3711e+01, -3.2061e+00,  1.8567e+01,\n",
      "         -2.4352e-01, -2.1152e+01,  2.0062e+01,  2.0565e+01,  4.3323e+01,\n",
      "          1.8368e+01,  5.5531e+00,  4.3091e+01,  1.1787e+01, -6.5391e+00,\n",
      "          8.8288e-01,  6.6553e+00, -2.0966e+00,  2.5484e+01, -1.0503e+01,\n",
      "         -1.3570e+01,  5.5202e+00,  1.1586e+01,  1.7529e+01,  7.1862e+00,\n",
      "         -3.0143e+01,  9.9905e+00,  1.2907e+01, -2.2816e+01,  2.5552e+01,\n",
      "         -3.7545e+00,  1.1366e+01, -3.3744e+01,  9.6308e+00, -1.3845e+01,\n",
      "          1.5923e+01, -2.0380e+01,  2.2175e+01, -6.9440e-01, -1.4012e+01,\n",
      "          2.5174e+01, -1.1912e+01,  1.8227e+01,  1.6872e+01,  3.6718e-02,\n",
      "          3.1080e+00, -2.3730e+01, -2.4898e+01, -1.1825e+01, -1.4257e+01,\n",
      "          7.2120e+00,  2.2216e+01,  7.0019e+00,  2.7784e+01,  1.4201e+01,\n",
      "          5.5883e-01,  1.1443e+01, -1.8585e+01,  7.0621e+00, -1.4211e+01,\n",
      "         -3.4449e+01, -1.9673e+00,  1.0677e+01,  2.3256e+01,  4.7109e+00,\n",
      "         -4.2989e+01,  1.5877e+01, -4.0590e+00,  3.1344e+01, -8.0661e+00,\n",
      "         -9.0089e+00],\n",
      "        [-4.4092e+00, -3.2980e+01,  1.4987e+01, -4.1363e+00, -7.5067e+00,\n",
      "         -1.1642e+01,  1.2342e+01,  2.9822e+01,  3.9696e+01, -1.3098e+01,\n",
      "          1.5484e+01, -2.3279e+01,  1.0768e+01,  2.9457e-01,  1.8048e+01,\n",
      "          1.6831e+01, -3.1063e+01,  2.5254e+01,  1.7688e+01,  3.2795e+01,\n",
      "         -1.6998e+01, -3.4372e+01,  1.9677e+01, -1.7472e+01,  4.1411e+00,\n",
      "         -3.2709e+00, -1.3340e-01, -6.0062e+00,  2.2169e+01,  6.1504e+00,\n",
      "         -3.5187e+01, -1.8341e+00, -1.6755e+01, -5.2821e+00, -7.3141e+00,\n",
      "          2.3403e+00, -1.1436e+01,  2.0664e+01, -1.4664e+01,  1.5190e+01,\n",
      "         -3.7104e+01,  1.3758e+01, -2.5951e+01,  1.0720e+00,  9.5010e+00,\n",
      "         -1.5079e+00, -1.2219e+01,  1.7948e+01,  2.0245e+01, -2.1029e+01,\n",
      "          1.1639e+01, -3.2963e+01,  6.4221e+00, -4.4061e+00,  2.5442e+01,\n",
      "          1.2322e+00, -5.6896e+00, -1.9806e+01,  3.4013e+00, -2.7578e+01,\n",
      "          3.4972e+01,  2.1999e+01, -1.0189e+01,  1.9838e+01,  3.8674e+01,\n",
      "          1.0808e+01, -1.7516e+01, -2.3372e+01,  2.9962e+01, -1.6350e+01,\n",
      "         -1.3629e+01, -9.5022e-01,  7.6935e+00,  2.4770e+01, -1.1678e+01,\n",
      "         -4.6909e+01,  1.7295e+01,  3.4155e+00,  1.8719e+01,  2.0597e+01,\n",
      "         -2.1821e+01],\n",
      "        [ 4.2932e+00,  3.6523e+01,  1.4646e+01, -3.2837e+01, -4.2854e+01,\n",
      "         -1.2265e+01, -2.9943e+01,  2.4165e+00,  4.8377e+01, -3.3978e+00,\n",
      "         -2.9479e+01,  1.7261e+01, -4.1730e+01,  2.9769e+01,  6.9061e+00,\n",
      "          4.4191e+01,  2.7958e+01,  1.5882e+01,  2.6254e+01,  2.8346e+01,\n",
      "          3.7628e+01,  4.1501e+00,  9.9481e+00,  2.8137e+01, -1.1883e+01,\n",
      "         -7.8401e-01, -4.2820e+01,  1.3404e+01,  3.1835e+01,  3.4419e+01,\n",
      "          3.2271e+01,  1.3548e+00,  1.9290e+01, -1.0159e+01, -4.1158e+01,\n",
      "         -4.4467e+01, -2.5537e+01,  6.2238e+00,  3.1055e+01, -1.5265e+01,\n",
      "          2.9005e+00, -1.0836e+01, -3.5164e+01,  3.3229e+01,  1.7331e+01,\n",
      "         -3.6956e+01, -2.8074e+01,  3.3642e+01, -4.7422e+00, -2.1394e+01,\n",
      "          6.2024e+00,  4.3046e+01, -1.1482e+01, -2.8105e+01,  5.4848e+00,\n",
      "         -8.6278e+00, -4.3123e+01, -2.6327e+01, -1.8452e+01,  2.0245e+01,\n",
      "          3.4008e+01, -1.3940e+01, -3.9309e+01, -9.9201e+00, -1.3358e+00,\n",
      "         -1.1672e+00,  6.8457e-01, -4.1435e+01, -1.8399e+01,  2.1790e+01,\n",
      "         -3.1276e+01, -2.6088e+01, -2.4686e+01, -5.6517e+00,  9.1538e+00,\n",
      "         -1.1416e+01, -6.2143e+00, -1.1890e+01, -4.8885e+00, -5.4583e+00,\n",
      "          3.8621e+01],\n",
      "        [-2.4164e+01, -2.9941e+01,  8.4728e+00, -1.4551e+00, -2.5508e+01,\n",
      "          2.5466e+01, -3.0956e+01, -3.5097e-01,  3.5988e+00, -4.7437e+00,\n",
      "         -8.6943e+00, -4.6178e+01, -8.6695e+00,  7.1261e+00, -8.5363e-01,\n",
      "         -8.2327e+00, -4.3258e+01,  4.1277e+01,  2.9854e+01,  4.9619e+00,\n",
      "          2.8809e+01, -1.6659e+01,  9.6166e+00, -7.6190e+00,  4.9424e+00,\n",
      "         -2.1623e+01, -1.8574e+00, -1.8454e+01,  2.7311e+01,  4.4768e+00,\n",
      "         -1.7739e+00,  6.1323e+00,  1.7357e+01,  2.0382e+01, -2.7534e+01,\n",
      "         -4.8695e+00, -3.5912e+01,  1.2468e+01, -3.1643e+00, -1.3980e+00,\n",
      "          1.4391e+01,  2.6959e+01,  1.8634e+01, -2.0567e+01,  1.3522e+01,\n",
      "         -7.9453e+00,  2.4681e+01,  1.9134e+01, -2.5807e+01, -2.0872e+01,\n",
      "          1.9079e+01,  2.2728e+01, -9.2103e+00,  2.1178e+01,  2.5041e+01,\n",
      "         -9.9431e+00, -3.5940e+01,  1.1306e+01, -2.3255e+01, -6.2823e+00,\n",
      "         -1.0227e+01,  3.4654e+01,  2.2560e+01, -4.6406e+00, -4.0846e-01,\n",
      "         -1.9338e+01, -2.2314e+00,  1.6553e+00,  3.8331e+01,  1.7663e+01,\n",
      "         -1.3402e+01,  3.0948e+01, -7.9303e+00,  1.4519e+01, -8.1452e+00,\n",
      "         -4.4668e+01, -9.3952e-01, -1.7805e+01,  9.5766e+00, -1.8671e+01,\n",
      "          3.3943e+00],\n",
      "        [-1.6411e+01, -3.2539e+00, -1.3762e+01,  1.8013e+01, -2.3966e+01,\n",
      "         -2.7527e+01, -1.5531e+01, -9.9575e+00,  3.7720e+01, -6.3367e+00,\n",
      "          1.4442e+01, -3.9926e+01,  2.0214e+01, -2.0430e+01,  3.9657e+01,\n",
      "          2.6073e+01,  1.7107e+01,  2.1726e+01, -1.3915e+01, -8.9021e+00,\n",
      "          2.9379e+01, -2.1407e+01,  3.8425e+01, -1.5589e+01, -2.6636e+01,\n",
      "          1.5507e+01,  7.1877e-01,  1.0028e+01,  4.5757e-01,  1.9781e+01,\n",
      "         -4.7458e+01, -2.0553e+01,  4.8583e+00, -2.5605e+01, -1.1436e+01,\n",
      "          2.9405e+00, -1.4312e+01,  1.2674e+01,  1.5765e+01, -1.0447e+01,\n",
      "         -2.2818e+00,  3.3192e+01,  1.3332e+01,  2.6891e+01,  2.1996e+01,\n",
      "         -3.0738e+01, -2.6221e+01,  3.7801e+01, -2.1200e+01,  7.6622e-01,\n",
      "          3.3249e+00, -8.3746e+00,  1.9918e+00,  1.0955e+01, -6.0657e+00,\n",
      "         -4.1795e-01,  6.9990e-01, -3.5681e+01, -2.6664e+01, -3.7703e+01,\n",
      "         -1.1607e+01,  3.3410e+01, -1.0542e+01,  3.4909e+01, -1.4435e+01,\n",
      "          1.1531e+01, -2.2084e+01,  2.0236e+01, -1.8382e+00,  7.5500e+00,\n",
      "         -1.6811e+01, -5.0382e+00, -1.5806e+01,  5.0768e+00, -2.5819e+01,\n",
      "         -3.4083e+01,  1.1883e+01, -7.4099e+00,  1.7497e+01,  1.8073e+01,\n",
      "          2.0920e+01],\n",
      "        [-7.6017e-01, -3.3664e+01,  2.8103e+01, -1.9893e+01,  1.6003e+01,\n",
      "          3.2859e+01,  3.1719e+01,  3.9666e+01, -1.9918e+01, -1.4390e+01,\n",
      "          1.4087e+01,  2.6185e+00, -1.4254e+01, -5.1741e+00,  2.8701e+00,\n",
      "          1.2246e+01, -4.3784e+01, -1.1688e+01,  8.1045e-01,  5.5069e+00,\n",
      "          1.4506e-01, -9.3546e+00,  3.3336e+01,  9.0991e+00,  6.6981e+00,\n",
      "          3.4871e+01,  2.3618e+01,  1.6495e+01, -3.4003e+01,  3.3539e+01,\n",
      "          8.5657e+00,  1.3300e+01,  2.6060e+01,  1.2031e+01, -1.5213e+00,\n",
      "          8.9607e+00, -1.1772e+01, -1.5506e+01, -4.7370e+00, -3.2270e+01,\n",
      "          1.5340e+01, -1.9946e+01, -2.7426e+00, -6.4356e+00,  2.1422e+01,\n",
      "          2.1826e+01,  2.7845e+01,  3.6549e+01,  4.1269e+01, -4.0087e+01,\n",
      "          4.2200e+01, -4.9614e-01,  6.0522e+00, -1.4028e+01,  2.0702e+01,\n",
      "          5.9786e+00, -2.7908e+01, -3.4101e+01, -8.3892e+00,  2.9324e+01,\n",
      "          4.8956e+00,  7.5285e-01,  1.6144e+01,  7.2256e+00,  9.7252e+00,\n",
      "         -3.5007e+01,  9.4049e-01,  5.3800e+00, -2.2410e+01, -4.5573e+01,\n",
      "         -3.4625e+01,  2.0237e+01, -3.0201e+00,  1.7065e+01,  3.5060e+01,\n",
      "         -4.6768e+01, -2.8955e+01, -2.1264e+01, -1.2287e+01,  2.0782e+01,\n",
      "          2.3795e+01],\n",
      "        [-1.9674e+01, -4.1773e+01,  2.6713e+01, -1.4531e+00, -2.7089e+01,\n",
      "         -1.0008e+01, -7.4353e-01,  3.1807e+01,  4.1256e+01, -6.6514e+00,\n",
      "          1.7884e+01, -4.8717e+01,  2.7193e+01, -4.0928e+00,  4.0738e+01,\n",
      "          3.2459e+01, -2.9582e+01,  4.3358e+01,  2.4055e+01,  4.0333e+01,\n",
      "         -3.9622e+00, -2.8752e+01,  4.8688e+01, -1.4777e+01,  1.5056e+01,\n",
      "         -8.3755e+00,  6.7276e+00, -1.7627e+01,  2.8901e+01, -1.1757e+01,\n",
      "         -4.4740e+01, -1.7667e+01, -4.8783e+00, -7.0067e+00, -4.6441e+00,\n",
      "         -1.6147e+01, -1.1389e+01,  1.2033e+01, -2.3097e+00,  3.7410e+01,\n",
      "         -3.9876e+01,  2.3981e+01, -2.2402e+01, -5.7278e-01,  8.2742e+00,\n",
      "         -4.5506e+00, -2.3208e+01,  3.2063e+01,  7.4216e+00, -2.8971e+01,\n",
      "          3.7365e+01, -4.0607e+01,  2.0992e+01,  8.2059e+00,  1.9375e+01,\n",
      "          1.8211e+01, -2.4128e+01, -4.4003e+01, -7.1706e+00, -2.0502e+01,\n",
      "          4.1201e+01,  3.6754e+01, -2.1247e+01,  1.6994e+01,  4.5162e+01,\n",
      "          1.8796e+01, -4.8246e+00, -2.2724e+01,  4.4248e+01, -2.1434e+01,\n",
      "         -2.5074e+01,  1.4926e+01,  1.7543e+01,  2.6546e+01, -7.9513e+00,\n",
      "         -4.8815e+01,  4.6135e+01, -1.5425e+01,  3.5206e+01,  1.5869e+01,\n",
      "         -1.4354e+01]], device='cuda:0', grad_fn=<ClampBackward0>)\n",
      "tensor(42095.9219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor([[-35.0211, -14.4883,   4.8140,  39.2977,   8.8765, -10.7859,  39.3844,\n",
      "          20.1772, -28.7527, -13.0359, -21.5096, -44.4949, -23.2532,   5.4218,\n",
      "         -11.8002,  37.1323,  -9.5465,  16.9715,  17.1171, -12.9405,  25.6766,\n",
      "         -40.1503, -46.4103,  12.4062,  40.8568, -41.4989, -49.8325,  19.4960,\n",
      "          20.8932, -35.6582,  41.6284,  42.6644,   3.4977, -12.5472,   9.5301],\n",
      "        [-31.3714,  41.4443, -22.8571,  -0.5840,   8.1087, -27.3941,  27.9564,\n",
      "          42.2610, -19.6486,  13.1124, -14.4261,  47.1081,  40.5514,  31.6947,\n",
      "          -0.2384, -16.8875,  37.1267,  41.2571,  49.1774,  30.9569, -10.3853,\n",
      "          19.2086,  27.4556,  39.4792,  34.8338, -31.2963, -27.6163, -35.1557,\n",
      "          -0.3294,  38.3725, -30.0251, -45.5547,  29.0348,  36.2910, -18.7606],\n",
      "        [ 49.8158, -15.5099, -41.5939,  43.2691,  38.1699,  -6.0289,  23.2954,\n",
      "         -11.9382,  22.1384, -17.6483,  44.4965,  21.3109,   4.8749,   4.7644,\n",
      "           0.5248,  -8.0773, -10.4113,  13.2941,  -7.9679,  29.5303,  31.1562,\n",
      "          14.6589, -13.5699,  11.3162,  17.1054,  26.6225, -39.3497, -27.5768,\n",
      "          23.4389,  29.5924,  43.4914, -16.8172,  48.9729,  -2.9397,   8.9738],\n",
      "        [ 43.3674, -36.8565, -22.1800, -43.8558,  -6.3786, -24.0298,  -2.5864,\n",
      "           4.5025,  28.6655, -18.6304,  -5.3495, -35.7631,  39.9308,  20.2633,\n",
      "          21.8005, -41.3988,  12.8272,  32.4138,  43.9269,  -7.8859,  28.9329,\n",
      "          45.7406,  38.3582, -31.7016,   9.4431, -28.8383, -28.8643,  27.9015,\n",
      "         -21.4629,   6.4358,  39.3063,  43.2490,  16.4979,   7.8236, -11.0417],\n",
      "        [ 29.6044,  22.4189, -14.9822,   1.1771, -14.6399, -49.1533,  21.4397,\n",
      "         -13.0076,  36.0957,  16.0515,  41.3138,  28.7194,   3.3766,   3.4959,\n",
      "          14.1325, -43.1510, -44.1286,   6.7876, -10.7745, -21.4165, -28.4703,\n",
      "          31.6003,  22.5285, -36.3991, -10.7219, -28.4335, -26.2817,  11.2640,\n",
      "         -34.5960, -41.8433,  43.6562,  41.6308,  -3.1848, -32.5073,  -9.9426],\n",
      "        [ 39.1167, -46.1557, -20.8222,  22.3582, -40.4089, -48.2030,  31.5086,\n",
      "         -17.3165,  48.2690,   1.8423,  -7.6684,  -3.8339,  -5.1466, -34.7874,\n",
      "          29.0058, -27.3333, -33.1051, -12.9274,  29.7491, -30.0499,  -2.0512,\n",
      "         -48.6896,  32.6029,  44.0254, -12.0378,  -9.8718, -45.7987,  28.7943,\n",
      "         -46.7481,  49.2753,  12.4250, -41.5570,   3.2446, -48.8193, -42.4957],\n",
      "        [ 35.9105, -14.4301, -39.8490,  35.5470,  15.9419,   0.9543,  29.3052,\n",
      "         -12.8383,  25.6108,  -8.8636,  31.6645,  26.9420,  -3.4728,  -8.1995,\n",
      "          -9.9338, -10.5094,   4.0836,   6.6837, -17.4940,  28.7680,  35.1868,\n",
      "          17.9916,  -1.0818,  -2.0905,   2.1603,  16.4761, -34.2046, -27.0592,\n",
      "          28.1614,  28.5638,  22.7885, -12.8062,  27.4459,   6.3856,   7.7457],\n",
      "        [-17.9049, -26.7049, -12.7924,  44.6619,  -9.9089, -32.1367,  32.1151,\n",
      "          41.3511, -20.6613,  41.1026, -24.5234, -36.7414,  10.5706,  15.2041,\n",
      "         -43.8713,  -3.1481,  25.9573,  43.6808, -31.6397,  -5.0622, -26.5175,\n",
      "         -46.2767,  -9.9527, -25.8175, -37.0205, -43.3960, -48.1019,  17.5565,\n",
      "          15.2178, -44.7730,  40.5467,  18.5145, -45.8892, -14.1885,  34.4266],\n",
      "        [ 47.5573, -12.0400, -32.3536,  24.8298,  24.3770,  -7.9784,   9.9929,\n",
      "           1.1348,  23.6670,  -4.9035,  40.2025,  19.5183,  -3.1996,  12.9565,\n",
      "          -8.4918,   2.8497,  -3.5452,   6.9355,   2.4928,  21.1541,  33.5711,\n",
      "          18.9311, -10.7965,   6.0691,  15.8799,  29.5607, -22.5581, -14.3468,\n",
      "          26.2232,  19.7056,  43.8568, -18.5620,  34.3733,  -7.0943,  13.8070],\n",
      "        [ 46.6513, -13.6431, -40.2465,  40.5081,  36.4128,  -4.8853,  23.7049,\n",
      "          -9.9289,  21.1833, -15.8857,  44.5755,  22.3004,   2.8164,   6.5552,\n",
      "          -0.6167,  -8.2199,  -9.4845,  12.3062,  -7.6053,  26.0654,  29.3742,\n",
      "          12.3788, -13.6263,   8.9464,  16.6284,  23.9927, -35.6695, -24.2284,\n",
      "          21.3491,  29.0632,  42.7386, -14.1991,  46.9665,  -4.2194,   9.3962]],\n",
      "       device='cuda:0', grad_fn=<ClampBackward0>)\n",
      "tensor(7.6509, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train(train_set, max_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, num_particle\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, padding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 102\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset, max_step, num_epochs, batch_size, hidden_dim, num_particle, padding_dim, device)\u001b[0m\n\u001b[0;32m     99\u001b[0m loss \u001b[38;5;241m=\u001b[39m mean_fitness\n\u001b[0;32m    101\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 102\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    105\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "train(train_set, max_step=50, num_epochs=1, batch_size=32, hidden_dim=16, num_particle=10, padding_dim=100, device='cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
