{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "- Agent: particles\n",
    "- Action: Choose w, c1, c2 through GNN\n",
    "- Enviroment: Function & The way PSO change state between step (episode)\n",
    "- State: (X, V, P, G)\n",
    "- Reward: - f(X)\n",
    "- Policy: GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from base_function import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSOEnvironment:\n",
    "    def __init__(self, num_particle, device, lower_bound=-5, upper_bound=5, max_steps=10):\n",
    "        self.device = torch.device(device)\n",
    "        self.num_particle = num_particle\n",
    "        self.lower_bound = torch.tensor(lower_bound, device=self.device)\n",
    "        self.upper_bound = torch.tensor(upper_bound, device=self.device)\n",
    "        self.max_steps = max_steps\n",
    "        self.batch = None\n",
    "        self.current_step = 0\n",
    "    \n",
    "    def set_batch(self, batch):\n",
    "        self.batch = batch\n",
    "        self.batch_size = len(batch)\n",
    "        self.dims = [f['dim'] for f in batch]\n",
    "        self.func_types = [f['func_type'] for f in batch]\n",
    "        self.params_list = [f['params'] for f in batch]\n",
    "    \n",
    "    def _initialize_population(self):\n",
    "        self.X = [torch.empty(self.num_particle, d, device=self.device).uniform_(self.lower_bound, self.upper_bound) for d in self.dims]\n",
    "        self.V = [torch.zeros_like(x) for x in self.X]\n",
    "        self.P = [x.clone() for x in self.X]\n",
    "        self.P_best = [torch.full((self.num_particle, 1), float('inf'), device=self.device) for _ in self.dims]\n",
    "        self.G = [x[0].clone().detach() for x in self.X]\n",
    "        self.global_best = [float('inf') for _ in self.dims]\n",
    "        self.fitnesses = [torch.full((self.num_particle, 1), float('inf'), device=self.device) for _ in self.dims]\n",
    "\n",
    "    def reset(self):\n",
    "        if not self.batch:\n",
    "            raise ValueError(\"Batch is not set. Call set_batch(batch) before reset().\")\n",
    "        self._initialize_population()\n",
    "        self.current_step = 0\n",
    "\n",
    "    def evaluate(self, x, func_type, params):\n",
    "        return Function.get_function(func_type, x, params, self.device).evaluate_function()\n",
    "\n",
    "    def function(self, x, func_type, params):\n",
    "        function_instance = Function.get_function(func_type, x, params, self.device)\n",
    "        return function_instance.evaluate_function().to(self.device)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            W = self.action[i, :, 0].unsqueeze(1).to(self.device)\n",
    "            C1 = self.action[i, :, 1].unsqueeze(1).to(self.device)\n",
    "            C2 = self.action[i, :, 2].unsqueeze(1).to(self.device)\n",
    "            \n",
    "            X = self.X[i].to(self.device)\n",
    "            V = self.V[i].to(self.device)\n",
    "            P = self.P[i].to(self.device)\n",
    "            \n",
    "            V_new = (W * V + C1 * (P - X) + C2 * (self.G[i] - X))\n",
    "\n",
    "            X_new = torch.clamp(X + V_new, self.lower_bound, self.upper_bound)\n",
    "\n",
    "            self.fitnesses[i] = self.function(X_new, self.func_types[i], self.params_list[i]).to(self.device)\n",
    "            fitnesses_no_grad = self.fitnesses[i].detach()\n",
    "            self.V[i] = V_new.detach()\n",
    "            self.X[i] = X_new.detach()\n",
    "\n",
    "            improve = fitnesses_no_grad < self.P_best[i]\n",
    "            self.P_best[i] = torch.where(improve, fitnesses_no_grad, self.P_best[i])\n",
    "            self.P[i] = torch.where(improve.reshape(-1, 1), self.X[i].detach(), self.P[i])\n",
    "\n",
    "            min_fitness, min_id = torch.min(fitnesses_no_grad, dim=0)\n",
    "            if self.G[i] is None or self.global_best[i] > min_fitness:\n",
    "                self.global_best[i] = min_fitness\n",
    "                self.G[i] = self.X[i][min_id].detach()\n",
    "        \n",
    "        reward = -torch.cat(self.fitnesses, dim=0).to(self.device)\n",
    "        reward = reward.view(self.batch_size, self.num_particle)\n",
    "        \n",
    "        return reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GNNPolicy(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(input_dims, hidden_dims)\n",
    "        self.conv2 = GCNConv(hidden_dims, hidden_dims)\n",
    "        # Lớp fc trả về 6 giá trị: 3 cho mu, 3 cho sigma (dạng 'thô' hoặc logits)\n",
    "        self.fc = nn.Linear(hidden_dims, 6)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: Tensor shape (B, n, d)\n",
    "            - B: batch_size\n",
    "            - n: số node / điểm trong mỗi graph\n",
    "            - d: số chiều đặc trưng cho mỗi node\n",
    "        edge_index: Tensor shape (2, E) mô tả các cạnh (trong torch_geometric)\n",
    "        \"\"\"\n",
    "        B, n, d = x.size()\n",
    "        x = x.view(B * n, d)\n",
    "        device = x.device\n",
    "\n",
    "        # Ghép edge_index cho B graph riêng thành 1 edge_index chung\n",
    "        edge_index_list = []\n",
    "        for i in range(B):\n",
    "            offset = i * n\n",
    "            edge_index_list.append(edge_index + offset)\n",
    "        edge_index_batch = torch.cat(edge_index_list, dim=1).to(device)\n",
    "\n",
    "        # GCN forward\n",
    "        x = F.relu(self.conv1(x, edge_index_batch))\n",
    "        x = F.relu(self.conv2(x, edge_index_batch))\n",
    "        x = self.fc(x)  # [B*n, 6]\n",
    "\n",
    "        # Tách thành mu (3) và sigma (3)\n",
    "        mu, raw_sigma = x.split(3, dim=-1)  # mỗi cái [B*n, 3]\n",
    "\n",
    "        # Đưa về [B, n, 3]\n",
    "        mu = mu.view(B, n, 3)\n",
    "        raw_sigma = raw_sigma.view(B, n, 3)\n",
    "\n",
    "        # Xử lý mu, sigma:\n",
    "        mu = torch.sigmoid(mu)  \n",
    "        sigma = torch.sigmoid(raw_sigma) * 0.5 + 1e-5  # đảm bảo sigma > 0\n",
    "\n",
    "        # Xử lý NaN (nếu có)\n",
    "        mu = torch.where(torch.isnan(mu), torch.zeros_like(mu), mu)\n",
    "        sigma = torch.where(torch.isnan(sigma), torch.zeros_like(sigma), sigma)\n",
    "\n",
    "        # Tạo ma trận covariance dạng chéo: (B, n, 3, 3)\n",
    "        cov = torch.diag_embed(sigma**2)\n",
    "\n",
    "        # Tạo phân phối MultivariateNormal 3D (mỗi node là 1 phân phối 3D)\n",
    "        dist_3d = MultivariateNormal(loc=mu, covariance_matrix=cov)\n",
    "        return dist_3d\n",
    "\n",
    "    def sample_action(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        - Trả về action shape (B, n, 3)\n",
    "        - log_prob shape (B, n)\n",
    "        \"\"\"\n",
    "        dist_3d = self.forward(x, edge_index)\n",
    "        action = dist_3d.rsample()         # (B, n, 3)\n",
    "        log_prob = dist_3d.log_prob(action)  # (B, n)\n",
    "        return action, log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "def train(dataloader, env, policy_net, optimizer, padding_dim, num_epochs, batch_size, num_steps, gamma, device):\n",
    "    max_grad_norm = 1.0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        num_batches = len(dataloader) // batch_size + (1 if len(dataloader) % batch_size > 0 else 0)\n",
    "        function_loader = create_batches(dataloader, batch_size)\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for batch_id, batch in enumerate(function_loader):\n",
    "            env.set_batch(batch)\n",
    "            env.reset()\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "            lambda_reg = 1e-4\n",
    "            for step in range(num_steps):\n",
    "                X_padded = torch.stack(padding_batch(env.X, batch, padding_dim), dim=0)\n",
    "                edge_index = torch.combinations(torch.arange(env.num_particle, device=device), r=2).t().contiguous().to(device)\n",
    "                action, log_prob = policy_net.sample_action(X_padded, edge_index)\n",
    "                log_probs.append(log_prob)\n",
    "                env.action = torch.sigmoid(action)\n",
    "                reward = env.step(env.action).to(device)\n",
    "                # print(f\"Step {step+1} - Raw Reward: {reward}\")\n",
    "                reward = (reward - reward.mean()) / (reward.std() + 1e-8)\n",
    "                rewards.append(reward)\n",
    "\n",
    "            discounted_rewards = []\n",
    "            cumulative_reward = torch.zeros_like(rewards[0])\n",
    "            for reward in reversed(rewards):\n",
    "                cumulative_reward = reward + gamma * cumulative_reward\n",
    "                discounted_rewards.insert(0, cumulative_reward)\n",
    "            discounted_rewards = torch.stack(discounted_rewards)\n",
    "            # print(f\"Discounted Rewards: {discounted_rewards}\")\n",
    "            # Normalize discounted rewards and make them non-negative\n",
    "            discounted_rewards = (discounted_rewards - discounted_rewards.mean(dim=0)) / (discounted_rewards.std(dim=0) + 1e-8)\n",
    "            discounted_rewards = discounted_rewards - discounted_rewards.min(dim=0, keepdim=True)[0]\n",
    "\n",
    "            loss_components = []\n",
    "            for lp, dr in zip(log_probs, discounted_rewards):\n",
    "                component_loss = (-lp * dr).mean()\n",
    "                loss_components.append(component_loss)\n",
    "                # print(f\"Log Prob: {lp}, Discounted Reward: {dr}, Component Loss: {component_loss}\")  # Debug components\n",
    "            loss = sum(loss_components)/len(loss_components)\n",
    "            # loss += lambda_reg * sum(p.pow(2).sum() for p in policy_net.parameters())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            print(f\"Batch {batch_id + 1}/{num_batches} - Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        epoch_loss = epoch_loss / batch_size\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f}, Time: {epoch_end_time - epoch_start_time:.2f}s\")\n",
    "    avg_loss = sum(epoch_losses) / num_epochs\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import torch.optim as optim\n",
    "\n",
    "def main():\n",
    "    splited_path = r'A:\\Code\\pso_rl\\splited_data_no.pkl'\n",
    "    with open(splited_path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "\n",
    "    train_set = dataset['train']\n",
    "    test_set = dataset['test']\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 64\n",
    "    num_steps = 25\n",
    "    padding_dim = 100\n",
    "    num_particle = 10\n",
    "    \n",
    "    # Khởi tạo môi trường RL\n",
    "    env = PSOEnvironment(num_particle=num_particle, device=device, lower_bound=-5, upper_bound=5, max_steps=num_steps)\n",
    "    policy_net = GNNPolicy(input_dims=padding_dim, hidden_dims=32).to(device)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "    num_epochs = 4\n",
    "    gamma = 0.99\n",
    "    \n",
    "    losses = train(\n",
    "        train_set, \n",
    "        env, \n",
    "        policy_net, \n",
    "        optimizer, \n",
    "        padding_dim, \n",
    "        num_epochs, \n",
    "        batch_size, \n",
    "        num_steps, \n",
    "        gamma, \n",
    "        device\n",
    "    )\n",
    "\n",
    "    print(\"Training completed. Losses:\", losses)\n",
    "\n",
    "    model_save_path = r'A:\\Code\\pso_rl\\data\\pso_rl_model.pth'\n",
    "    torch.save(policy_net.state_dict(), model_save_path)\n",
    "    print(f\"Model saved at {model_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from statistics import mean, stdev\n",
    "import pickle\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def load_model(model_path, policy_net):\n",
    "    try:\n",
    "        state_dict = torch.load(model_path)\n",
    "        policy_net.load_state_dict(state_dict)\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model: {e}\")\n",
    "        raise e\n",
    "    return policy_net\n",
    "\n",
    "def test(test_set, env, policy_net, padding_dim, num_steps, gamma, device):\n",
    "    import collections\n",
    "    from statistics import mean, stdev\n",
    "\n",
    "    func_rewards = { 'ackley': [], 'levy': [], 'largeman': [], 'griewank': [], 'styblinski_tang':[]}\n",
    "    step_convergence = { 'ackley': [], 'levy': [], 'largeman': [], 'griewank': [], 'styblinski_tang': []}\n",
    "\n",
    "    function_loader = create_batches(test_set, batch_size=32)\n",
    "\n",
    "    for batch_id, batch in enumerate(function_loader):\n",
    "        env.set_batch(batch)\n",
    "        env.reset()\n",
    "        log_probs = []\n",
    "        cumulative_rewards = collections.defaultdict(list)\n",
    "        final_step_rewards = collections.defaultdict(list)\n",
    "        no_improve_steps = collections.defaultdict(int)\n",
    "        converged_func_types = set()  # Initialize outside the loop\n",
    "        prev_global_best = None  # Initialize outside the loop\n",
    "        max_no_improve = 6  # Termination condition if no improvement for 6 steps\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            X_padded = torch.stack(padding_batch(env.X, batch, padding_dim), dim=0)\n",
    "            edge_index = torch.combinations(torch.arange(env.num_particle, device=device), r=2).t().contiguous()\n",
    "            action, log_prob = policy_net.sample_action(X_padded, edge_index)\n",
    "            log_probs.append(log_prob)\n",
    "            env.action = torch.sigmoid(action)\n",
    "            reward = env.step(env.action).to(device)\n",
    "\n",
    "            # Initialize prev_global_best after the first step\n",
    "            if step == 0:\n",
    "                prev_global_best = [env.global_best[i].item() for i in range(len(env.func_types))]\n",
    "                continue  # Skip convergence check for step 0\n",
    "\n",
    "            # Check for convergence for each func_type\n",
    "            for i, func_type in enumerate(env.func_types):\n",
    "                if func_type in converged_func_types:  # Skip if already converged\n",
    "                    continue\n",
    "\n",
    "                current_best = env.global_best[i].item()\n",
    "                if current_best >= prev_global_best[i]:  # No improvement (we are minimizing)\n",
    "                    no_improve_steps[func_type] += 1\n",
    "                else:\n",
    "                    no_improve_steps[func_type] = 0\n",
    "\n",
    "                if no_improve_steps[func_type] >= max_no_improve:\n",
    "                    # print(f\"Convergence detected for {func_type} at step {step}. Early stopping for this function type.\")\n",
    "                    if func_type not in step_convergence or len(step_convergence[func_type]) == 0:\n",
    "                        step_convergence[func_type].append(step)\n",
    "                    converged_func_types.add(func_type)  # Mark as converged\n",
    "\n",
    "            prev_global_best = [env.global_best[i].item() for i in range(len(env.func_types))]\n",
    "\n",
    "            # Optional: Early stopping if all function types have converged\n",
    "            if len(converged_func_types) == len(env.func_types):\n",
    "                break\n",
    "\n",
    "            # Track rewards for each function type at each step\n",
    "            for i, func_type in enumerate(env.func_types):\n",
    "                reward_values = reward[i].tolist()  # Convert tensor to list of scalars\n",
    "                cumulative_rewards[func_type].extend(reward_values)\n",
    "                if step == num_steps - 1 or no_improve_steps[func_type] >= max_no_improve:\n",
    "                    final_step_rewards[func_type].extend(reward_values)\n",
    "\n",
    "        # Store the final global_best as the best reward for the batch\n",
    "        for i, func_type in enumerate(env.func_types):\n",
    "            func_rewards[func_type].append(env.global_best[i].item())\n",
    "\n",
    "    # Log final statistics\n",
    "    print(\"\\nFinal Statistics:\")\n",
    "    for func_type in func_rewards:\n",
    "        avg_reward = mean(func_rewards[func_type])\n",
    "        std_reward = stdev(func_rewards[func_type]) if len(func_rewards[func_type]) > 1 else 0.0\n",
    "        avg_convergence = mean(step_convergence[func_type]) if step_convergence[func_type] else -1\n",
    "        print(f\"Function {func_type}: Average Reward: {avg_reward:.4f} ± {std_reward:.4f}, Average Convergence Step: {avg_convergence:.2f}\")\n",
    "\n",
    "    # Calculate best and worst results from final timestep rewards\n",
    "    best_rewards = {ftype: max(values) for ftype, values in final_step_rewards.items()}\n",
    "    worst_rewards = {ftype: min(values) for ftype, values in final_step_rewards.items()}\n",
    "\n",
    "    print(\"\\nBest Results (Final Timestep):\")\n",
    "    for func_type, reward in best_rewards.items():\n",
    "        print(f\"Function {func_type}: Best Reward: {reward:.4f}\")\n",
    "\n",
    "    print(\"\\nWorst Results (Final Timestep):\")\n",
    "    for func_type, reward in worst_rewards.items():\n",
    "        print(f\"Function {func_type}: Worst Reward: {reward:.4f}\")\n",
    "\n",
    "    convergence_results = {ftype: -max(values) for ftype, values in func_rewards.items()}\n",
    "    print(\"\\nConvergence Results:\")\n",
    "    for func_type, convergence in convergence_results.items():\n",
    "        print(f\"Function {func_type}: {convergence:.4f}\")\n",
    "\n",
    "    print(\"\\nDimensional Analysis (from env.dims):\")\n",
    "    print(f\"Dimensions: {env.dims}\")\n",
    "\n",
    "    return {func_type: (mean(func_rewards[func_type]), stdev(func_rewards[func_type]) if len(func_rewards[func_type]) > 1 else 0.0) for func_type in func_rewards}\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    set_seed(625)\n",
    "    gamma = 0.99\n",
    "    num_steps = 50\n",
    "    padding_dim = 100\n",
    "    model_path = r\"A:\\Code\\pso_rl\\data\\pso_rl_model.pth\"\n",
    "    splited_path = r'A:\\Code\\pso_rl\\splited_data_no.pkl'\n",
    "\n",
    "    # Load dataset\n",
    "    with open(splited_path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    train_set = dataset['train']\n",
    "    test_set = dataset['test']\n",
    "\n",
    "    # Initialize environment and model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_particle = 10\n",
    "    env = PSOEnvironment(num_particle=num_particle, device=device, lower_bound=-5, upper_bound=5, max_steps=num_steps)\n",
    "    policy_net = GNNPolicy(input_dims=padding_dim, hidden_dims=32).to(device)\n",
    "\n",
    "    # Load the trained model\n",
    "    policy_net = load_model(model_path, policy_net)\n",
    "\n",
    "    # Test the model\n",
    "    best_rewards = test(\n",
    "        test_set=test_set, \n",
    "        env=env, \n",
    "        policy_net=policy_net, \n",
    "        padding_dim=padding_dim, \n",
    "        num_steps=num_steps, \n",
    "        gamma=gamma, \n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(best_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
