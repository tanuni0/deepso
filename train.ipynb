{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import PSOGNN\n",
    "from pso import PSO \n",
    "from base_function import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splited_path = r'A:\\Code\\deepso\\splited_data_no.pkl'\n",
    "with open(splited_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dim': 1, 'func_type': 'ackley', 'params': [16.98135401917914, 0.28507902333427554, 5.580495503135101]}\n"
     ]
    }
   ],
   "source": [
    "train_set = dataset['train']\n",
    "# val_set = dataset['validation']\n",
    "test_set = dataset['test']\n",
    "print(train_set[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(dataset, batch_size): \n",
    "    random.shuffle(dataset)\n",
    "    for i in range(0, len(dataset), batch_size): \n",
    "        batch = dataset[i:i + batch_size]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_amp = torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = 4\n",
    "num_epochs = 10\n",
    "UB = 1\n",
    "LB = 0\n",
    "num_particles = 100\n",
    "\n",
    "def save_model(model, optimizer, epoch, path=\"model_checkpoint.pth\"):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, path)\n",
    "    print(f\"Model saved at epoch {epoch} to {path}\")\n",
    "\n",
    "def load_model(model, optimizer, path=\"model_checkpoint.pth\"):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded from {path}, resuming from epoch {epoch}\")\n",
    "    return epoch\n",
    "\n",
    "\n",
    "def train(dataloader, accumulate_step, save_path=\"model_checkpoint.pth\"):\n",
    "    epoch_losses = []\n",
    "    batch_losses = {}\n",
    "    function_losses = {}\n",
    "    function_loss_ranges = {}\n",
    "\n",
    "    patience = 5  \n",
    "    min_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    max_norm = 1.0  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        function_loader = create_batches(dataloader, batch_size)\n",
    "\n",
    "        for batch in function_loader:\n",
    "            for i in range(0, len(batch), mini_batch_size):\n",
    "                mini_batch = batch[i : i + mini_batch_size]\n",
    "                for func in mini_batch:\n",
    "                    dim = func['dim']\n",
    "                    func_type = func['func_type']\n",
    "                    params = func['params']\n",
    "                    model = PSOGNN(node_input_dim=dim).to(device)\n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "                    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "                    scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "                    def function(x):\n",
    "                        function_instance = Function.get_function(func_type, x, params)\n",
    "                        return function_instance.evaluate_function()\n",
    "\n",
    "                    X = torch.rand(num_particles, dim).to(device)\n",
    "\n",
    "                    Pso = PSO(X, function, model, LB, UB)\n",
    "                    position_best, best, mean_fitness = Pso.run()\n",
    "\n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast('cuda'): \n",
    "                            loss = mean_fitness / len(mini_batch) \n",
    "                            loss = loss / accumulate_step  \n",
    "                            scaler.scale(loss).backward()  \n",
    "                    else:\n",
    "                        loss = mean_fitness\n",
    "                        loss = loss / accumulate_step\n",
    "                        loss.backward()\n",
    "\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "                    if (i + 1) % accumulate_step == 0:\n",
    "                        if use_amp:\n",
    "                            scaler.step(optimizer)\n",
    "                            scaler.update()\n",
    "                        else:\n",
    "                            optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    if func_type not in function_losses:\n",
    "                        function_losses[func_type] = []\n",
    "                    function_losses[func_type].append(loss.item())\n",
    "\n",
    "                    if epoch not in batch_losses:\n",
    "                        batch_losses[epoch] = []\n",
    "                    batch_losses[epoch].append(loss.item())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        scheduler.step(avg_loss)  \n",
    "\n",
    "        if avg_loss < min_loss:\n",
    "            min_loss = avg_loss\n",
    "            epochs_no_improve = 0 \n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        save_model(model, optimizer, epoch+1, save_path)\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Average Loss')\n",
    "    plt.title('Average Loss over Epochs')\n",
    "    plt.show()\n",
    "\n",
    "    for func_type, losses in function_losses.items():\n",
    "        min_loss = min(losses)\n",
    "        max_loss = max(losses)\n",
    "        function_loss_ranges[func_type] = (min_loss, max_loss)\n",
    "        print(f\"Function Type: {func_type}, Loss Range: Min={min_loss:.4f}, Max={max_loss:.4f}\")\n",
    "\n",
    "    return function_loss_ranges\n",
    "\n",
    "\n",
    "\n",
    "def test(dataloader, model_path=\"model_checkpoint.pth\"):\n",
    "\n",
    "    test_loss = 0\n",
    "    batch_count = 0\n",
    "    all_test_losses = []\n",
    "\n",
    "    function_loader = create_batches(dataloader, batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in function_loader:\n",
    "            for mini_batch in batch:\n",
    "                dim = mini_batch['dim']\n",
    "                func_type = mini_batch['func_type']\n",
    "                params = mini_batch['params']\n",
    "                model = PSOGNN(node_input_dim=dim).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "                load_model(model, optimizer, model_path)\n",
    "                model.eval()\n",
    "                \n",
    "                def function(x):\n",
    "                    function_instance = Function.get_function(func_type, x, params)\n",
    "                    return function_instance.evaluate_function()\n",
    "\n",
    "                X = torch.rand(num_particles, dim).to(device)\n",
    "                \n",
    "                Pso = PSO(X, function, model, LB, UB)\n",
    "                position_best, best, mean_fitness = Pso.run()\n",
    "\n",
    "                loss = mean_fitness\n",
    "                all_test_losses.append(loss.item())\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                batch_count += 1\n",
    "\n",
    "    avg_test_loss = test_loss / batch_count if batch_count > 0 else 0\n",
    "    print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "    # Plot test losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(all_test_losses, marker='o')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss per Batch on Test Set')\n",
    "    plt.show()\n",
    "\n",
    "    return avg_test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "accumulate_step = 4 \n",
    "\n",
    "print(\"Starting Training...\")\n",
    "train_loss_ranges = train(train_set, accumulate_step, save_path=\"model_checkpoint.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Testing...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'dim' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Testing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m avg_test_loss \u001b[38;5;241m=\u001b[39m test(test_set, model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_checkpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 138\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(dataloader, model_path)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(dataloader, model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_checkpoint.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 138\u001b[0m     model \u001b[38;5;241m=\u001b[39m PSOGNN(node_input_dim\u001b[38;5;241m=\u001b[39mdim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    139\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# Load the saved model\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'dim' where it is not associated with a value"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nStarting Testing...\")\n",
    "avg_test_loss = test(test_set, model_path=\"model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Assuming 'train_dataloader' and 'test_dataloader' are provided\n",
    "accumulate_step = 4  # Example accumulate step\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "train_loss_ranges = train(train_set, accumulate_step)\n",
    "\n",
    "print(\"\\nStarting Testing...\")\n",
    "avg_test_loss = test(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Start fresh or load from a saved model\n",
    "# model, optimizer, start_epoch, epoch_losses, val_losses = load_checkpoint(MODEL_PATH)\n",
    "\n",
    "# Call to train (either fresh or continue from checkpoint)\n",
    "train(train_set, val_set, accumulate_step=4, start_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# mini_batch_size = 4\n",
    "# num_epochs = 10\n",
    "\n",
    "# UB = 1\n",
    "# LB = 0\n",
    "# num_particles = 100\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def train(dataloader, accumulate_step):\n",
    "#     epoch_losses = []\n",
    "#     batch_losses = {}\n",
    "#     function_losses = {}\n",
    "#     function_loss_ranges = {}\n",
    "\n",
    "#     scaler = torch.cuda.amp.GradScaler()  # Initialize the GradScaler for mixed precision\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "#         total_loss = 0\n",
    "#         batch_count = 0\n",
    "#         function_loader = create_batches(dataloader, batch_size)\n",
    "\n",
    "#         for batch in function_loader:\n",
    "#             for i in range(0, len(batch), mini_batch_size):\n",
    "#                 mini_batch = batch[i : i + mini_batch_size]\n",
    "#                 for func in mini_batch:\n",
    "#                     dim = func['dim']\n",
    "#                     func_type = func['func_type']\n",
    "#                     params = func['params']\n",
    "\n",
    "#                     def function(x):\n",
    "#                         function_instance = Function.get_function(func_type, x, params)\n",
    "#                         return function_instance.evaluate_function()\n",
    "\n",
    "#                     X = torch.rand(num_particles, dim).to(device)\n",
    "#                     model = PSOGNN(node_input_dim=dim).to(device)\n",
    "#                     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#                     Pso = PSO(X, function, model, LB, UB)\n",
    "#                     position_best, best, mean_fitness = Pso.run()\n",
    "\n",
    "#                     with torch.cuda.amp.autocast():  # Mixed precision training\n",
    "#                         loss = mean_fitness / len(mini_batch)  # Normalize by batch size\n",
    "#                         loss = loss / accumulate_step  # Scale by accumulate step\n",
    "#                         scaler.scale(loss).backward()  # Scale and backpropagate\n",
    "\n",
    "#                     if (i + 1) % accumulate_step == 0:\n",
    "#                         scaler.step(optimizer)  # Apply optimizer step\n",
    "#                         scaler.update()\n",
    "#                         optimizer.zero_grad()\n",
    "\n",
    "#                     if func_type not in function_losses:\n",
    "#                         function_losses[func_type] = []\n",
    "#                     function_losses[func_type].append(loss.item())\n",
    "\n",
    "#                     if epoch not in batch_losses:\n",
    "#                         batch_losses[epoch] = []\n",
    "#                     batch_losses[epoch].append(loss.item())\n",
    "\n",
    "#                     if torch.isinf(loss).any() or torch.isnan(loss).any():\n",
    "#                         print(f\"Warning: Loss is {'-inf' if torch.isinf(loss).any() else 'NaN'} at Epoch {epoch+1}, Step {i+1}\")\n",
    "#                         break\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "#             if torch.isinf(torch.tensor(total_loss)) or torch.isnan(torch.tensor(total_loss)):\n",
    "#                 print(f\"Warning: total_loss is {'-inf' if torch.isinf(torch.tensor(total_loss)) else 'NaN'} at Epoch {epoch+1}, Step {i+1}\")\n",
    "#                 break\n",
    "#             batch_count += 1\n",
    "\n",
    "#         avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "#         epoch_losses.append(avg_loss)\n",
    "\n",
    "#         if torch.isinf(torch.tensor(avg_loss)) or torch.isnan(torch.tensor(avg_loss)):\n",
    "#             print(f\"Warning: avg_loss is {'-inf' if torch.isinf(torch.tensor(avg_loss)) else 'NaN'} at Epoch {epoch+1}\")\n",
    "\n",
    "#         print(f\"Avg Loss Epoch {epoch+1}: {avg_loss:.4f}\")\n",
    "\n",
    "#     print(\"Training complete!\")\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Average Loss')\n",
    "#     plt.title('Average Loss over Epochs')\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Print the min-max range for each function type\n",
    "#     for func_type, losses in function_losses.items():\n",
    "#         min_loss = min(losses)\n",
    "#         max_loss = max(losses)\n",
    "#         function_loss_ranges[func_type] = (min_loss, max_loss)\n",
    "#         print(f\"Function Type: {func_type}, Loss Range: Min={min_loss:.4f}, Max={max_loss:.4f}\")\n",
    "\n",
    "#     return function_loss_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "# # Initialize parameters\n",
    "# batch_size = 16\n",
    "# mini_batch_size = 4\n",
    "# num_epochs = 15\n",
    "# UB = 1\n",
    "# LB = 0\n",
    "# num_particles = 100\n",
    "\n",
    "# # Path to save the model\n",
    "# MODEL_PATH = r'A:\\Code\\deepso\\check_point.pth'\n",
    "\n",
    "# # Define the evaluation function\n",
    "# def evaluate(dataloader, device):\n",
    "#     total_loss = 0\n",
    "#     batch_count = 0\n",
    "\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for batch in create_batches(dataloader, batch_size):\n",
    "#             for i in range(0, len(batch), mini_batch_size):\n",
    "#                 mini_batch = batch[i: i + mini_batch_size]\n",
    "#                 for func in mini_batch:\n",
    "#                     dim = func['dim']  # Get the dimension from the function data\n",
    "#                     func_type = func['func_type']\n",
    "#                     params = func['params']\n",
    "\n",
    "#                     # Define the function\n",
    "#                     def function(x):\n",
    "#                         function_instance = Function.get_function(func_type, x, params)\n",
    "#                         return function_instance.evaluate_function()\n",
    "                    \n",
    "#                     # Re-create the model for each 'func' based on the 'dim'\n",
    "#                     model = PSOGNN(node_input_dim=dim).to(device)\n",
    "\n",
    "#                     # Initialize PSO with this model\n",
    "#                     X = torch.rand(num_particles, dim).to(device)\n",
    "#                     Pso = PSO(X, function, model, LB, UB)\n",
    "#                     _, _, mean_fitness = Pso.run()\n",
    "\n",
    "#                     loss = mean_fitness\n",
    "#                     total_loss += loss.item()\n",
    "#                     batch_count += 1\n",
    "\n",
    "#     avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "#     print(f\"Evaluation Avg Loss: {avg_loss:.4f}\")\n",
    "#     return avg_loss\n",
    "\n",
    "\n",
    "# # Define the training function with model saving\n",
    "# def train(dataloader, validation_loader, accumulate_step, start_epoch = 0):\n",
    "#     epoch_losses = []  # To store average loss for each epoch\n",
    "#     val_losses = []    # To store validation loss for each epoch\n",
    "#     function_losses = {}  # To store loss per function type\n",
    "#     batch_losses = {}  # To store batch loss per epoch\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     for epoch in range(start_epoch, num_epochs):\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "#         total_loss = 0\n",
    "#         batch_count = 0\n",
    "#         batch_losses[epoch] = []  # Initialize batch loss tracking for this epoch\n",
    "\n",
    "#         for batch in create_batches(dataloader, batch_size):\n",
    "#             for i in range(0, len(batch), mini_batch_size):\n",
    "#                 mini_batch = batch[i: i + mini_batch_size]\n",
    "#                 for func in mini_batch:\n",
    "#                     dim = func['dim']  # Get the dimension from the function data\n",
    "#                     func_type = func['func_type']\n",
    "#                     params = func['params']\n",
    "\n",
    "#                     # Define the function\n",
    "#                     def function(x):\n",
    "#                         function_instance = Function.get_function(func_type, x, params)\n",
    "#                         return function_instance.evaluate_function()\n",
    "\n",
    "#                     # Re-create the model for each 'func' based on the 'dim'\n",
    "#                     model = PSOGNN(node_input_dim=dim).to(device)\n",
    "#                     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#                     # Initialize PSO with this model\n",
    "#                     X = torch.rand(num_particles, dim).to(device)\n",
    "#                     Pso = PSO(X, function, model, LB, UB)\n",
    "#                     position_best, best, mean_fitness = Pso.run()\n",
    "\n",
    "#                     loss = mean_fitness / accumulate_step\n",
    "#                     loss.backward()\n",
    "\n",
    "#                     if func_type not in function_losses:\n",
    "#                         function_losses[func_type] = []  # Initialize list if func_type is not tracked\n",
    "#                     function_losses[func_type].append(loss.item())  # Track loss per function type\n",
    "                    \n",
    "#                     batch_losses[epoch].append(loss.item())  # Track loss per batch for this epoch\n",
    "\n",
    "#                     if (i + 1) % accumulate_step == 0:\n",
    "#                         optimizer.step()\n",
    "#                         optimizer.zero_grad()\n",
    "\n",
    "#                     # Check for invalid loss values\n",
    "#                     if torch.isinf(loss).any() or torch.isnan(loss).any():\n",
    "#                         print(f\"Warning: Loss is {'-inf' if torch.isinf(loss).any() else 'NaN'} at Epoch {epoch+1}, Step {i+1}\")\n",
    "#                         break\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "#             batch_count += 1\n",
    "        \n",
    "#         avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "#         epoch_losses.append(avg_loss)  # Track average loss for this epoch\n",
    "        \n",
    "#         if torch.isinf(torch.tensor(avg_loss)) or torch.isnan(torch.tensor(avg_loss)):\n",
    "#             print(f\"Warning: avg_loss is {'-inf' if torch.isinf(torch.tensor(avg_loss)) else 'NaN'} at Epoch {epoch+1}\")\n",
    "        \n",
    "#         print(f\"Avg Loss Epoch {epoch+1}: {avg_loss:.4f}\")\n",
    "\n",
    "        \n",
    "#         # Evaluate on the validation set after each epoch\n",
    "#         val_loss = evaluate(validation_loader, device)\n",
    "#         val_losses.append(val_loss)\n",
    "\n",
    "#         # Save model checkpoint after each epoch\n",
    "#         save_checkpoint(model, optimizer, epoch, epoch_losses, val_losses, MODEL_PATH)\n",
    "\n",
    "#     print(\"Training complete!\")\n",
    "\n",
    "#     # Plot average loss over epochs\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', label='Training Loss')\n",
    "#     plt.plot(range(1, num_epochs + 1), val_losses, marker='o', label='Validation Loss', color='red')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Average Loss')\n",
    "#     plt.title('Average Loss over Epochs')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Plot loss per function type across batches\n",
    "#     for func_type, losses in function_losses.items():\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(losses, marker='o')\n",
    "#         plt.xlabel('Batch')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title(f'Loss per Batch for {func_type}')\n",
    "#         plt.show()\n",
    "\n",
    "#     # Plot batch loss per epoch\n",
    "#     for epoch, losses in batch_losses.items():\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(losses, marker='o')\n",
    "#         plt.xlabel('Batch')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title(f'Loss per Batch for Epoch {epoch+1}')\n",
    "#         plt.show()\n",
    "\n",
    "# # Save checkpoint function\n",
    "# def save_checkpoint(model, optimizer, epoch, epoch_losses, val_losses, path):\n",
    "#     torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'epoch_losses': epoch_losses,\n",
    "#         'val_losses': val_losses\n",
    "#     }, path)\n",
    "#     print(f\"Model saved at {path}\")\n",
    "\n",
    "# # Load checkpoint function\n",
    "# def load_checkpoint(path):\n",
    "#     if os.path.exists(path):\n",
    "#         checkpoint = torch.load(path)\n",
    "#         model = PSOGNN(node_input_dim=dim).to(device)\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         start_epoch = checkpoint['epoch'] + 1\n",
    "#         epoch_losses = checkpoint['epoch_losses']\n",
    "#         val_losses = checkpoint['val_losses']\n",
    "#         print(f\"Loaded model from {path}, starting from epoch {start_epoch}\")\n",
    "#         return model, optimizer, start_epoch, epoch_losses, val_losses\n",
    "#     else:\n",
    "#         print(f\"No checkpoint found at {path}\")\n",
    "#         return None, None, 0, [], []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train(train_set, accumulate_step=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# batch_size = 16\n",
    "# mini_batch_size = 4\n",
    "# num_particles = 100\n",
    "# UB = 1\n",
    "# LB = 0\n",
    "\n",
    "# def test(test_loader):\n",
    "#     total_loss = 0\n",
    "#     batch_count = 0\n",
    "#     batch_losses = []  # Track loss per batch\n",
    "#     function_losses = {}  # Track loss per function type\n",
    "\n",
    "#     with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "#         for batch in create_batches(test_loader, batch_size):\n",
    "#             for i in range(0, len(batch), mini_batch_size):\n",
    "#                 mini_batch = batch[i: i + mini_batch_size]\n",
    "#                 for func in mini_batch:\n",
    "#                     dim = func['dim']\n",
    "#                     func_type = func['func_type']\n",
    "#                     params = func['params']\n",
    "\n",
    "#                     def function(x):\n",
    "#                         function_instance = Function.get_function(func_type, x, params)\n",
    "#                         return function_instance.evaluate_function()\n",
    "\n",
    "#                     X = torch.rand(num_particles, dim).to(device)\n",
    "\n",
    "#                     # Khởi tạo mô hình dựa trên kích thước đầu vào của hàm func\n",
    "#                     model = PSOGNN(node_input_dim=dim).to(device)\n",
    "\n",
    "#                     # Không cần optimizer ở đây vì không có cập nhật trọng số\n",
    "#                     Pso = PSO(X, function, model, LB, UB)\n",
    "#                     position_best, best, mean_fitness = Pso.run()\n",
    "\n",
    "#                     loss = mean_fitness\n",
    "\n",
    "#                     if func_type not in function_losses:\n",
    "#                         function_losses[func_type] = []  # Initialize if not tracked\n",
    "#                     function_losses[func_type].append(loss.item())  # Track loss per function type\n",
    "\n",
    "#                     batch_losses.append(loss.item())  # Track batch losses\n",
    "\n",
    "#                     total_loss += loss.item()\n",
    "#                     batch_count += 1\n",
    "\n",
    "#     avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "#     print(f\"Test Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     # Plot loss per function type across batches\n",
    "#     for func_type, losses in function_losses.items():\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(losses, marker='o')\n",
    "#         plt.xlabel('Batch')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title(f'Test Loss per Batch for {func_type}')\n",
    "#         plt.show()\n",
    "\n",
    "#     # Plot batch loss for the whole test set\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(batch_losses, marker='o')\n",
    "#     plt.xlabel('Batch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title('Test Loss per Batch')\n",
    "#     plt.show()\n",
    "\n",
    "#     return avg_loss\n",
    "\n",
    "# # Example call to test\n",
    "# test_set = dataset['test']\n",
    "# test(test_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "# # Initialize parameters\n",
    "# batch_size = 16\n",
    "# mini_batch_size = 4\n",
    "# num_epochs = 15\n",
    "# UB = 1\n",
    "# LB = 0\n",
    "# num_particles = 100\n",
    "\n",
    "# # Path to save the model\n",
    "# MODEL_PATH = r'A:\\Code\\deepso\\check_point.pth'\n",
    "\n",
    "# # Define the evaluation function\n",
    "# def evaluate(dataloader, device):\n",
    "#     total_loss = 0\n",
    "#     batch_count = 0\n",
    "\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for batch in create_batches(dataloader, batch_size):\n",
    "#             for i in range(0, len(batch), mini_batch_size):\n",
    "#                 mini_batch = batch[i: i + mini_batch_size]\n",
    "#                 for func in mini_batch:\n",
    "#                     dim = func['dim']  # Get the dimension from the function data\n",
    "#                     func_type = func['func_type']\n",
    "#                     params = func['params']\n",
    "\n",
    "#                     # Define the function\n",
    "#                     def function(x):\n",
    "#                         function_instance = Function.get_function(func_type, x, params)\n",
    "#                         return function_instance.evaluate_function()\n",
    "                    \n",
    "#                     # Re-create the model for each 'func' based on the 'dim'\n",
    "#                     model = PSOGNN(node_input_dim=dim).to(device)\n",
    "\n",
    "#                     # Initialize PSO with this model\n",
    "#                     X = torch.rand(num_particles, dim).to(device)\n",
    "#                     Pso = PSO(X, function, model, LB, UB)\n",
    "#                     _, _, mean_fitness = Pso.run()\n",
    "\n",
    "#                     loss = mean_fitness\n",
    "#                     total_loss += loss.item()\n",
    "#                     batch_count += 1\n",
    "\n",
    "#     avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "#     print(f\"Evaluation Avg Loss: {avg_loss:.4f}\")\n",
    "#     return avg_loss\n",
    "\n",
    "\n",
    "# # Define the training function with model saving\n",
    "# def train(dataloader, validation_loader, accumulate_step, start_epoch = 0):\n",
    "#     epoch_losses = []  # To store average loss for each epoch\n",
    "#     val_losses = []    # To store validation loss for each epoch\n",
    "#     function_losses = {}  # To store loss per function type\n",
    "#     batch_losses = {}  # To store batch loss per epoch\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#     for epoch in range(start_epoch, num_epochs):\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "#         total_loss = 0\n",
    "#         batch_count = 0\n",
    "#         batch_losses[epoch] = []  # Initialize batch loss tracking for this epoch\n",
    "\n",
    "#         for batch in create_batches(dataloader, batch_size):\n",
    "#             for i in range(0, len(batch), mini_batch_size):\n",
    "#                 mini_batch = batch[i: i + mini_batch_size]\n",
    "#                 for func in mini_batch:\n",
    "#                     dim = func['dim']  # Get the dimension from the function data\n",
    "#                     func_type = func['func_type']\n",
    "#                     params = func['params']\n",
    "\n",
    "#                     # Define the function\n",
    "#                     def function(x):\n",
    "#                         function_instance = Function.get_function(func_type, x, params)\n",
    "#                         return function_instance.evaluate_function()\n",
    "\n",
    "#                     # Re-create the model for each 'func' based on the 'dim'\n",
    "#                     model = PSOGNN(node_input_dim=dim).to(device)\n",
    "#                     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "#                     # Initialize PSO with this model\n",
    "#                     X = torch.rand(num_particles, dim).to(device)\n",
    "#                     Pso = PSO(X, function, model, LB, UB)\n",
    "#                     position_best, best, mean_fitness = Pso.run()\n",
    "\n",
    "#                     loss = mean_fitness / accumulate_step\n",
    "#                     loss.backward()\n",
    "\n",
    "#                     if func_type not in function_losses:\n",
    "#                         function_losses[func_type] = []  # Initialize list if func_type is not tracked\n",
    "#                     function_losses[func_type].append(loss.item())  # Track loss per function type\n",
    "                    \n",
    "#                     batch_losses[epoch].append(loss.item())  # Track loss per batch for this epoch\n",
    "\n",
    "#                     if (i + 1) % accumulate_step == 0:\n",
    "#                         optimizer.step()\n",
    "#                         optimizer.zero_grad()\n",
    "\n",
    "#                     # Check for invalid loss values\n",
    "#                     if torch.isinf(loss).any() or torch.isnan(loss).any():\n",
    "#                         print(f\"Warning: Loss is {'-inf' if torch.isinf(loss).any() else 'NaN'} at Epoch {epoch+1}, Step {i+1}\")\n",
    "#                         break\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "#             batch_count += 1\n",
    "        \n",
    "#         avg_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "#         epoch_losses.append(avg_loss)  # Track average loss for this epoch\n",
    "        \n",
    "#         if torch.isinf(torch.tensor(avg_loss)) or torch.isnan(torch.tensor(avg_loss)):\n",
    "#             print(f\"Warning: avg_loss is {'-inf' if torch.isinf(torch.tensor(avg_loss)) else 'NaN'} at Epoch {epoch+1}\")\n",
    "        \n",
    "#         print(f\"Avg Loss Epoch {epoch+1}: {avg_loss:.4f}\")\n",
    "\n",
    "        \n",
    "#         # Evaluate on the validation set after each epoch\n",
    "#         val_loss = evaluate(validation_loader, device)\n",
    "#         val_losses.append(val_loss)\n",
    "\n",
    "#         # Save model checkpoint after each epoch\n",
    "#         save_checkpoint(model, optimizer, epoch, epoch_losses, val_losses, MODEL_PATH)\n",
    "\n",
    "#     print(\"Training complete!\")\n",
    "\n",
    "#     # Plot average loss over epochs\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.plot(range(1, num_epochs + 1), epoch_losses, marker='o', label='Training Loss')\n",
    "#     plt.plot(range(1, num_epochs + 1), val_losses, marker='o', label='Validation Loss', color='red')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Average Loss')\n",
    "#     plt.title('Average Loss over Epochs')\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Plot loss per function type across batches\n",
    "#     for func_type, losses in function_losses.items():\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(losses, marker='o')\n",
    "#         plt.xlabel('Batch')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title(f'Loss per Batch for {func_type}')\n",
    "#         plt.show()\n",
    "\n",
    "#     # Plot batch loss per epoch\n",
    "#     for epoch, losses in batch_losses.items():\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(losses, marker='o')\n",
    "#         plt.xlabel('Batch')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title(f'Loss per Batch for Epoch {epoch+1}')\n",
    "#         plt.show()\n",
    "\n",
    "# # Save checkpoint function\n",
    "# def save_checkpoint(model, optimizer, epoch, epoch_losses, val_losses, path):\n",
    "#     torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'epoch_losses': epoch_losses,\n",
    "#         'val_losses': val_losses\n",
    "#     }, path)\n",
    "#     print(f\"Model saved at {path}\")\n",
    "\n",
    "# # Load checkpoint function\n",
    "# def load_checkpoint(path):\n",
    "#     if os.path.exists(path):\n",
    "#         checkpoint = torch.load(path)\n",
    "#         model = PSOGNN(node_input_dim=dim).to(device)\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         start_epoch = checkpoint['epoch'] + 1\n",
    "#         epoch_losses = checkpoint['epoch_losses']\n",
    "#         val_losses = checkpoint['val_losses']\n",
    "#         print(f\"Loaded model from {path}, starting from epoch {start_epoch}\")\n",
    "#         return model, optimizer, start_epoch, epoch_losses, val_losses\n",
    "#     else:\n",
    "#         print(f\"No checkpoint found at {path}\")\n",
    "#         return None, None, 0, [], []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
